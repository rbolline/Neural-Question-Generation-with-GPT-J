Roberta e=2,b=8

***** Running training *****
  Num examples = 3781
  Num Epochs = 2
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 946

{'loss': 0.4073, 'learning_rate': 2.357293868921776e-05, 'epoch': 1.06}
Saving model checkpoint to ./results/roberta_difficulty_ep=2_batch=8/checkpoint-500
Configuration saved in ./results/roberta_difficulty_ep=2_batch=8/checkpoint-500/config.json
Model weights saved in ./results/roberta_difficulty_ep=2_batch=8/checkpoint-500/pytorch_model.bin
tokenizer config file saved in ./results/roberta_difficulty_ep=2_batch=8/checkpoint-500/tokenizer_config.json
Special tokens file saved in ./results/roberta_difficulty_ep=2_batch=8/checkpoint-500/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 815.7599, 'train_samples_per_second': 9.27, 'train_steps_per_second': 1.16, 'train_loss': 0.35374489395109343, 'epoch': 2.0}
***** Running Evaluation *****
  Num examples = 222
  Batch size = 8
{'eval_loss': 0.3820355534553528, 'eval_accuracy': 0.8873873873873874, 'eval_f1': 0.8579945244735563, 'eval_precision': 0.8598765432098765, 'eval_recall': 0.856175542205478, 'eval_runtime': 7.9528, 'eval_samples_per_second': 27.915, 'eval_steps_per_second': 3.521, 'epoch': 2.0}
EVAL METRICS:

Reusing dataset race (/home/rb4987/.cache/huggingface/datasets/race/all/0.1.0/5839ff74a429622f5f20cca69c5fcf0e87ac6d5fd2777c42b948000684829f7b)
100%|█████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 364.67it/s]
***** Running Prediction *****
  Num examples = 197
  Batch size = 8
PREDICTION METRICS:

PredictionOutput(predictions=array([[-2.719082  ,  2.4034956 ],
       [-2.698978  ,  2.382507  ],
       [-2.5806282 ,  2.3047025 ],
       [-2.613077  ,  2.3322086 ],
       [ 1.1637459 , -1.1536349 ],
       [ 1.1643611 , -1.1542115 ],
       [ 1.1426646 , -1.1287129 ],
       [-2.7185788 ,  2.3983233 ],
       [-2.688335  ,  2.3827355 ],
       [ 0.54625475, -0.51364285],
       [-1.7353026 ,  1.646938  ],
       [-2.4084656 ,  2.1928186 ],
       [-2.687209  ,  2.385448  ],
       [-2.7094722 ,  2.3920095 ],
       [-2.5028749 ,  2.2489305 ],
       [ 1.1660755 , -1.1561846 ],
       [-2.6319842 ,  2.3358998 ],
       [ 1.1556182 , -1.1442361 ],
       [-2.5873826 ,  2.3122241 ],
       [-2.6612637 ,  2.3566039 ],
       [-2.2164707 ,  2.041392  ],
       [ 1.1633227 , -1.1528409 ],
       [-2.2708519 ,  2.0760589 ],
       [ 1.1630856 , -1.1526752 ],
       [-2.7127225 ,  2.3973556 ],
       [ 1.1533324 , -1.1413537 ],
       [ 1.167836  , -1.1582812 ],
       [-2.6268744 ,  2.3349073 ],
       [-2.722084  ,  2.4067695 ],
       [-2.647199  ,  2.3499691 ],
       [-2.7098386 ,  2.3989327 ],
       [-2.5354772 ,  2.276939  ],
       [-2.6806092 ,  2.3720367 ],
       [-2.6694129 ,  2.3859148 ],
       [-2.6838686 ,  2.3662066 ],
       [ 1.1522707 , -1.1399885 ],
       [ 1.1535989 , -1.1417694 ],
       [-2.6233687 ,  2.3337824 ],
       [-2.524197  ,  2.260189  ],
       [-2.4222577 ,  2.1906478 ],
       [ 0.4998703 , -0.46385407],
       [-2.64933   ,  2.3476048 ],
       [-2.4897432 ,  2.2336485 ],
       [-2.697261  ,  2.3909438 ],
       [ 1.1639295 , -1.1535697 ],
       [-2.5668883 ,  2.3089445 ],
       [-2.4572833 ,  2.234894  ],
       [-2.6639962 ,  2.367006  ],
       [-2.5715537 ,  2.2877939 ],
       [ 1.1684159 , -1.1590658 ],
       [-2.5894127 ,  2.3152752 ],
       [-2.6946075 ,  2.3891    ],
       [-1.9584016 ,  1.8272268 ],
       [-2.6170979 ,  2.3242762 ],
       [-2.7645934 ,  2.454041  ],
       [-2.60745   ,  2.3340635 ],
       [-2.534179  ,  2.2732787 ],
       [ 1.1594988 , -1.1484702 ],
       [-2.3742595 ,  2.155314  ],
       [-2.6469615 ,  2.35101   ],
       [-2.378991  ,  2.1414666 ],
       [ 1.1667153 , -1.156964  ],
       [-2.7106833 ,  2.3966143 ],
       [-0.08651296,  0.155177  ],
       [-2.4813397 ,  2.2291195 ],
       [-2.6275656 ,  2.3292654 ],
       [-2.6582108 ,  2.3521423 ],
       [ 1.1644487 , -1.1543475 ],
       [-2.6847017 ,  2.3772311 ],
       [ 1.165753  , -1.1557924 ],
       [ 0.93600786, -0.9100299 ],
       [ 1.1642059 , -1.1540836 ],
       [-1.5015789 ,  1.3917626 ],
       [-2.7373314 ,  2.4155197 ],
       [-2.3228374 ,  2.1345153 ],
       [-2.6753535 ,  2.3816996 ],
       [-2.661547  ,  2.3621566 ],
       [-2.6376255 ,  2.3350918 ],
       [ 1.1579276 , -1.1468238 ],
       [ 1.1532143 , -1.141266  ],
       [-2.703296  ,  2.3872404 ],
       [-2.4140544 ,  2.1851673 ],
       [-2.6740606 ,  2.3658736 ],
       [-2.6048002 ,  2.3236482 ],
       [ 0.15262038, -0.08776959],
       [-2.1263232 ,  1.9845029 ],
       [-2.66558   ,  2.36676   ],
       [-2.6401465 ,  2.3472738 ],
       [ 1.1664071 , -1.1565714 ],
       [ 1.1575058 , -1.1464108 ],
       [ 1.166142  , -1.1562477 ],
       [-2.7208786 ,  2.4077547 ],
       [-2.4326587 ,  2.2039204 ],
       [-2.4244654 ,  2.1909037 ],
       [-2.6872914 ,  2.3786294 ],
       [-2.182499  ,  2.022677  ],
       [ 1.1567007 , -1.1454133 ],
       [ 1.1659448 , -1.1557484 ],
       [-1.9141473 ,  1.7955501 ],
       [ 1.1600344 , -1.1494179 ],
       [-2.5346303 ,  2.2869396 ],
       [ 1.1601417 , -1.1494163 ],
       [-2.406623  ,  2.1949124 ],
       [-2.6978378 ,  2.3841875 ],
       [-2.4473736 ,  2.2101727 ],
       [-2.6923163 ,  2.3840272 ],
       [-2.696014  ,  2.3800921 ],
       [ 1.1653565 , -1.155374  ],
       [ 1.1618961 , -1.1514271 ],
       [-2.710157  ,  2.4075987 ],
       [-2.7500136 ,  2.435347  ],
       [-2.561116  ,  2.2951698 ],
       [-2.6760273 ,  2.3789856 ],
       [-2.5948107 ,  2.3199635 ],
       [-2.6974037 ,  2.3897777 ],
       [ 1.1551869 , -1.1437235 ],
       [ 1.1581548 , -1.1471051 ],
       [-2.6763468 ,  2.377098  ],
       [-2.1731608 ,  2.0281239 ],
       [ 1.1532227 , -1.1414967 ],
       [ 1.1519834 , -1.1400411 ],
       [ 1.1638565 , -1.1536175 ],
       [-2.3125503 ,  2.1018496 ],
       [-2.2171943 ,  2.0416896 ],
       [-2.6312134 ,  2.3294156 ],
       [-1.7186661 ,  1.6326051 ],
       [-2.6880288 ,  2.38051   ],
       [ 1.0068995 , -0.9837031 ],
       [-2.670336  ,  2.359014  ],
       [ 1.1667855 , -1.1569779 ],
       [-2.6402378 ,  2.34184   ],
       [ 1.1285602 , -1.1145691 ],
       [-2.691219  ,  2.382757  ],
       [ 1.1660422 , -1.1560541 ],
       [-2.4287052 ,  2.1882718 ],
       [-2.6970074 ,  2.390823  ],
       [ 1.1650932 , -1.1550287 ],
       [ 1.1649575 , -1.1548799 ],
       [-2.673641  ,  2.3668978 ],
       [-2.6214337 ,  2.3351889 ],
       [-2.6575913 ,  2.3673618 ],
       [ 1.161231  , -1.1507877 ],
       [-2.348182  ,  2.1232498 ],
       [-1.9996964 ,  1.8584623 ],
       [-2.2197375 ,  2.0300276 ],
       [ 1.1439612 , -1.1302079 ],
       [ 1.1541634 , -1.1423603 ],
       [ 1.1633432 , -1.153163  ],
       [ 1.1664506 , -1.1565497 ],
       [-2.3661685 ,  2.1426876 ],
       [-2.5763237 ,  2.3146386 ],
       [-2.1832466 ,  2.0019767 ],
       [-2.4686048 ,  2.2424486 ],
       [-2.7210543 ,  2.4083638 ],
       [-1.7684507 ,  1.6694642 ],
       [-2.4082556 ,  2.208674  ],
       [ 1.1536919 , -1.1420305 ],
       [ 1.1604344 , -1.1496373 ],
       [-2.6820066 ,  2.3757932 ],
       [ 1.1674721 , -1.1578938 ],
       [ 0.93658596, -0.91505796],
       [-2.3916764 ,  2.1613326 ],
       [-0.46014673,  0.41122034],
       [-2.2698362 ,  2.075545  ],
       [-2.679333  ,  2.375676  ],
       [-2.5025618 ,  2.2479632 ],
       [-2.5575402 ,  2.2820263 ],
       [-2.419096  ,  2.1925385 ],
       [-2.699736  ,  2.387705  ],
       [-1.8356655 ,  1.7403237 ],
       [ 1.1452967 , -1.1319588 ],
       [-2.6751685 ,  2.3675277 ],
       [ 0.95941323, -0.9187972 ],
       [ 1.1656079 , -1.1556689 ],
       [-2.5380602 ,  2.2769132 ],
       [-2.6630936 ,  2.3519614 ],
       [-2.2482462 ,  2.0685914 ],
       [ 1.1507324 , -1.1385435 ],
       [-2.0717678 ,  1.9404696 ],
       [-2.524864  ,  2.2659152 ],
       [-2.616479  ,  2.3410852 ],
       [-2.0548549 ,  1.9067881 ],
       [-2.1847887 ,  2.033744  ],
       [-1.7782806 ,  1.6669813 ],
       [-2.7683098 ,  2.4575822 ],
       [-2.290594  ,  2.0813076 ],
       [-2.6876433 ,  2.3798344 ],
       [-2.6774988 ,  2.3754883 ],
       [ 1.1645392 , -1.1543021 ],
       [-2.6910546 ,  2.373851  ],
       [-2.4784591 ,  2.2369077 ],
       [-1.2344539 ,  1.1645062 ],
       [-2.2846124 ,  2.0835288 ],
       [-2.653961  ,  2.3661003 ],
       [-2.7155402 ,  2.4023561 ],
       [-1.9809705 ,  1.8825095 ],
       [-2.6868753 ,  2.3801498 ]], dtype=float32), label_ids=array([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), metrics={'test_loss': 0.42754077911376953, 'test_accuracy': 0.8578680203045685, 'test_f1': 0.8214193214193215, 'test_precision': 0.8088563631853138, 'test_recall': 0.8387755102040817, 'test_runtime': 7.0352, 'test_samples_per_second': 28.002, 'test_steps_per_second': 3.554})
