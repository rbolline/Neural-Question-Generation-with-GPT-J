% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Zero-shot Question Generation with GPT-J}

% Author information can be set in various styles:

% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\

% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Sukhrit Rao \\
  Center for Data Science \\
  New York University \\
  \texttt{str8775@nyu.edu} \\\And
  Rohit Bollineni \\
  Center for Data Science \\
  New York University \\
  \texttt{rb4987@nyu.edu} \\\And
  Hasan Khan \\
  Courant Institute \\ 
  New York University \\
  \texttt{hk3550@nyu.edu} \\}

\begin{document}
\maketitle
\begin{abstract}
The last thing you figure out in writing a [paper] is what to put first - Pascal
\end{abstract}

\section{Introduction}

Asking good questions forms an essential part of assessing a student’s grasp of certain concepts. Instructors spend considerable time constructing exam and assignment questions that assess students on material taught in the classroom. Questions often need to be continuously replaced over time as questions become shared or publicly available. Often, human instructors may not come up with the clearest questions \citep{human-eval:7}. Neural Question Generation (QG) systems aim to automate the process of question construction by generating novel questions given a particular context, thus reducing time and costs of question generation for educators and test developers. Advanced QG systems with configurable parameters could help offer students custom material based on their individual ability, and act as a foundation for adaptive testing and learning \citep{adaptive-education:8}. 

Recent work in QG has focused on generating quiz-style questions \citep{Quiz:1}, with particular focus on generating questions of selected difficulty levels \citep{Difficulty:3}. However, these techniques have relied on fine-tuning a language model on a task-specific dataset such as SQuAD or RACE. These models are limited in their domain of use, cumbersome to train, and do not generalize out-of-distribution (citation???). Moreover, constructing datasets to train such models is time-consuming and costly, and thus not a viable means for widespread adoption. In this paper, we propose using GPT-J in a zero-shot setting to produce questions that are fluent in linguistic construction, relevant to the input context, and appropriately difficult as requested in the input. We compare our work against a baseline GPT-J model fine-tuned on the task.

In the context of reading comprehension, we look at two types of QG variants. In answer-focused QG, a reference passage and an answer are passed as inputs into the system, resulting in the generation of questions relevant to the input answer. In general QG, only a context passage is passed as input, resulting in the generation of unmapped questions relevant to the context.

\section{Method}

\subsection{Data}

We use RACE \citep{RACE:2}, a compilation of reading comprehension questions from middle and high school English exams administered to Chinese students, to fine tune and prompt our GPT-J model. Questions that are generic (i.e less than 5 words, non-specific to the context, etc.) are dropped, alongside with any questions with numeric answers, in order to prevent the model from generating vague questions (???). 

For the answer-focused setting, training inputs to the QG model are composed of a question, an answer, a context and a difficulty. We determine difficulty by mapping middle school questions to \textit{easy} and high school questions to \textit{hard} difficulties.   ---- data points are used for fine-tuning, and 1695 for testing. The same 1695 datapoints are used for prompting in the zero shot setting. We only use contexts that have at least two associated questions, in order to allow for zero shot prompting for every test context. A similar input is used for the general setting, except no answer is included.  ---- data points are used for general fine-tuning, and ---- for general prompting in the zero shot setting. 

We experiment with various prompts for the zero shot setting (insert future prompt variant info here). An example of the zero shot input prompt is shown below: 

\begin{quote}
\textbf{Context}: Although most weddings follow long-held traditions, there's still room for American individualism. For example, the usual place for a wedding is in a church. But some people get married outdoors in a scenic spot. A few even have the ceremony while skydiving or riding on horseback! The couple may invite hundreds of people or just a few close friends. They choose their own style of colors, decorations and music during the ceremony. But some things rarely change. The bride usually wears a beautiful long white wedding dress. She traditionally wears "something old, something new, something borrowed and something blue." The groom wears a formal suit.

\textbf{Difficulty}: Hard. 

\textbf{Answer}: Some people choose their own style of weddings. 

\textbf{Question}: <generated question>

\end{quote}

In this instance, the true reference question is "Which of the following best shows American individualism?"

\subsection{Model \& Tools}

For generating questions, we select GPT-J-6B \citep{gpt-j:4} pulled from HuggingFace for its manageable parameter size (6 billion) and open-source code, allowing us to fine-tune the model. We use the following model parameter values: 

\begin{table}[h]
\centering
\begin{tabular}{lc}
\hline
\textbf{Parameter} & \textbf{Value}\\
\hline
do-sample & True \\
temperature & 1.0 \\
max-new-tokens & 250 \\
top-k & 50 \\ 
top-p & 1.0  \\ 
length-penalty & 0.8 \\
return-dict-in-generate & False \\
\hline
\end{tabular}
\end{table}

For difficulty classification, we use RoBERTa.
We perform model training on NYU GCP. All code can be found in our \href{https://github.com/rbolline/Neural-Question-Generation-with-GPT-J}{Github repository}. 



\subsection{Evaluation}

Generated questions are evaluated on three criteria: fluency, relevancy, and difficulty. Generally, we use a mix of manual human-centric evaluation, the gold standard in evaluating outputs from NLG systems \citep{human-eval:7}, and limited use of automated model based evaluations.

\textbf{Fluency} determines weather a question is easy to read and understand, without taking the source (passage or reference question) into account, and is manually evaluated on a 0-2 Likert scale (ranging from poor to excellent). \textbf{Relevancy} \cite{relevancy:5} determines whether the generated questions and input questions are topically related, and measured using manual evaluation on a 0-2 Likert scale (ranging from relevant to irrelevant) as well as with evaluation metrics including BLEU (which we use to measures N-gram overlap between reference text and generated text) \citep{bleu:12}, BLEURT (which uses a transformer model trained to score the similarity between a reference and hypothesis text) \citep{bleurt:11} and ROUGE-L (which measures the recall and precision of longest common subsequence between a reference and hypothesis text) \citep{rouge:13}. \textbf{Difficulty} is determined by checking if the generated question’s difficulty matches that of the input difficulty fed to the model, and is measured in an automated way using a GPT-J classifier trained on RACE to classify questions as easy or hard. We compare the classifier's output difficulty with the requested difficulty from the input, and evaluate the F1 score.  

\section{Results}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\hline
\textbf{Metric} & \textbf{Finetuned} & \textbf{Zeroshot}\\
\hline
Fluency & 0.0 & 0.0 \\
Relevancy & 0.0 & 0.0 \\
Difficulty & 0.0 & 82.0 \\
\hline
\end{tabular}
\end{table}

The table above show our results for GPT-J outputs in the zeroshot and finetuned settings on the metrics of fluency, relevancy and difficulty as defined in Evaluations. 

\begin{table*}
\centering
\begin{tabular}{lll}
\hline
\textbf{Reference Question} & \textbf{Answer} & \textbf{Generated Question}\\
\hline
* & * & * \\
\hline
* & * & * \\
\hline
* & * & * \\
\hline
* & * & * \\
\hline
\end{tabular}
\caption{\label{citation-guide}
Questions generated by GPT-J in a zero shot setting
}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{lll}
\hline
\textbf{Reference Question} & \textbf{Answer} & \textbf{Generated Question}\\
\hline
* & * & * \\
\hline
* & * & * \\
\hline
* & * & * \\
\hline
* & * & * \\
\hline
\end{tabular}
\caption{\label{citation-guide}
Questions generated by finetuned GPT-J 
}
\end{table*}

\section{Discussion}

\section{Collaboration}
Credit to Sukhrit Rao for formulating the project question. He wrote and ran the training and inference scripts for both GPT-J question generating models, prompt design, and evaluation metrics.
Credit to Rohit Bollineni for writing the training script, building the difficulty classification model
Credit to Hasan Khan for question preprocessing, paper writing, and pulling references together. All team members worked on manual evaluations of the questions generated 



\subsection{References}

\nocite{Ando2005,borschinger-johnson-2011-particle,andrew2007scalable,rasooli-tetrault-2015,goodman-etal-2016-noise,harper-2014-learning}

The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{custom}
\end{verbatim}
\end{quote}

You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
To include both the Anthology and your own .bib file, use the following instead of the above.
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{anthology,custom}
\end{verbatim}
\end{quote}

Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

\subsection{Appendices}

Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

\section{Bib\TeX{} Files}
\label{sec:bibtex}

Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section*{Acknowledgements}

This document has been adapted
by Steven Bethard, Ryan Cotterell and Rui Yan
from the instructions for earlier ACL and NAACL proceedings, including those for 
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan, 
NAACL 2017 by Margaret Mitchell, 
ACL 2012 by Maggie Li and Michael White, 
ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
ACL 2002 by Eugene Charniak and Dekang Lin, 
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{citations}
\bibliographystyle{acl_natbib}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is an appendix.

\end{document}