% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Few-Shot Question Generation with GPT-J}

% Author information can be set in various styles:

% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\

% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Sukrit Rao \\
  Center for Data Science \\
  New York University \\
  \texttt{str8775@nyu.edu} \\\And
  Rohith Bollineni \\
  Center for Data Science \\
  New York University \\
  \texttt{rb4987@nyu.edu} \\\And
  Hasan Khan \\
  Courant Institute \\ 
  New York University \\
  \texttt{hk3550@nyu.edu} \\}

\begin{document}
\maketitle
\begin{abstract}
Neural Question Generation (QG) systems aim to automate the process of question construction by generating novel questions given a particular context, thus reducing time and costs of question generation for educators and test developers. We propose Question Generation using GPT-J in a few-shot setting. Generating questions in this manner reduces time and resource cost required to construct datasets and fine-tune increasingly complex models like GPT-J, thereby increasing usage for educational purposes such as adaptive education. We compare our results against a GPT-J model fine-tuned on the task. 
\end{abstract}

\section{Introduction}

Asking relevant questions of varying difficulty forms an essential part of assessing a student’s grasp of concepts. Instructors are required to spend considerable amounts of time constructing exam and assignment questions that assess students on material taught in the classroom.
In this process of question creation, instructors must make sure that the questions cannot all be of similar difficulty and must range from easy to difficult to ensure optimum learning outcomes. Additionally, questions often need to be replaced continually as content is revised to reflect the latest updates in the domain or because the questions become publicly available. Another aspect that affects learning outcomes is the student's aptitude and pre-existing knowledge. In classroom settings, learning takes place in groups in which some students would be stronger in the concepts the material tests while others would not and would require additional practice and/or instruction. As a result, applying universal pedagogy in such settings is not ideal \citep{personalized-learning:9}.  Adaptive education systems have great potential to improve learning outcomes by increasing accessibility \citep{adaptive-education:8}. 

Question Generation (QG) systems \citep{nqg:15} aim to automate the process of question construction by generating novel questions given a particular context, thus reducing time and costs of question generation for educators and test developers. Advanced QG systems with configurable parameters could help offer students custom material based on their individual ability, and act as a foundation for adaptive testing and learning. 

Recent work in QG has focused on generating quiz-style questions \citep{Quiz:1}, with particular focus on generating questions of selected difficulty levels \citep{Difficulty:3}. However, these techniques have relied on fine-tuning a language model on a task-specific dataset such as SQuAD \citep{squad:11} or RACE \citep{RACE:2}. As a result, these models are limited in their domain of use. Moreover, constructing such datasets that contain thousands of examples, specific to that task, is time-consuming and costly, and thus not a viable means for widespread adoption. In this paper, we propose using GPT-J in a few-shot setting to produce questions that are fluent in linguistic construction, relevant to the input context, and appropriately difficult as desired. We compare our work against a GPT-J model fine-tuned on the task.

In the context of reading comprehension, we look at two types of QG variants. In answer-focused QG, a reference passage and an answer are passed as inputs into the system, resulting in the generation of questions relevant to the input answer. In general QG, only a context passage is passed as input, resulting in the generation of unmapped questions relevant to the context.

\section{Related Work}

\subsection{Difficulty Controllable Question Generation}

DQG discusses a framework to generate questions given a difficulty level along with the context and answer. Bidirectional LSTMs are used to encode the input. For the decoder, another LSTM is used along with a global difficulty control variable trained on ground truth difficulty labels. The authors use R-Net and BiDAF to assess the difficulty of a question; if both models guess correctly then it is labeled easy; if both models fail then it is labeled hard. 

\subsection{Quiz Style Question Generation}
As mentioned above, \citet{Quiz:1} focuses on quiz style questions as part of the NewsQuizQA dataset. Unlike prior datasets like SQuAD, prior context does not have a strong impact on the NewsQuizQA dataset with regards to generating questions which reference source text. NewsQuizQA implements Minimum Reference Loss to judge its models, allowing for multiple correct outputs based on how well results match with QA pairs. This metric style allows for some open ended behavior from the model.

\subsection{Evaluation of Text Generation: A Survey}
This paper \citep{text-gen-survey:13} serves as a framework for evaluation metrics used to judge machine generated text. The authors delineate three categories of evaluation. Human-centric evaluation involves humans (often experts) judging the quality of generated text, usually through manual review. Untrained automatic evaluation focuses on comparing text generated by models with human written text (such as BLEU, BLEURT and ROUGE).  Machine-learned evaluation involved a machine learning model acting in place of a human judge to determine similarities in human vs. machine (or machine vs. machine) generated texts. All three categories of evaluation are used in our paper, discussed further in the evalatuation section.


\section{Methodology}

\subsection{Data}

We use RACE \citep{RACE:2}, a compilation of reading comprehension questions from middle and high school English exams administered to Chinese students, to fine-tune and prompt our GPT-J model. We choose RACE since ``the difficulty of RACE questions mostly comes from the understanding of the story but not from the way how the question is asked'' \citep{Difficulty:3} (p.2), which is what one finds in the education domain. Moreover, RACE categorizes questions into either \textit{middle} or \textit{high}, based on their relative difficulty. As a result, no manual labeling is required to obtain difficulty labels for questions.

Questions that are generic (i.e less than 5 words, non-specific to the context, etc.) are dropped in order to prevent the model from generating irrelevant questions.

For the answer-focused setting, training inputs to the QG model are composed of a question, an answer, a context and a desired difficulty of the generated question. We determine difficulty by mapping middle school questions to \textit{easy} and high school questions to \textit{hard} difficulties. 31051 data points are used for fine-tuning, and 1695 for testing. We only use context passages that have at least two associated questions, in order to allow for the prompt to contain at least one example for every test context. A similar input is used for the general setting, except no answer is included. 

We experiment with various prompts for the few-shot setting (insert future prompt variant info here). An example of the input prompt used in the few-shot setting is shown below: 

% environment
\begin{small}

\begin{quote}
\textbf{Context}: ``Family'' is of course an elastic word. But when British people say that their society is based on family life, they are thinking of ``family'' in its narrow, peculiarly European sense of mother ,father and children living together alone in their own house as an economic and social unit. Thus, every British marriage indicates the beginning of a new and independent family--hence the tremendous importance of marriage in British life. Readers of novels like Jane Austen's Pride and Prejudice will know that in former times marriage among wealthy families were arranged by the girl's parents, that is, it was the parents' duty to find a suitable husband for their daughter, preferably a rich one, and by skillful encouragement to lead him eventually to ask their permission to marry her. 

\textbf{Difficulty}: Hard. 

\textbf{Answer}: It gives quite some idea of English social life in the past.

\textbf{Question}: What is true concerning the book Pride and Prejudice?

\textbf{Difficulty}: Hard. 

\textbf{Answer}: Different definitions could be given to the word.

\textbf{Question}: 

\end{quote}

\end{small}


In this instance, the true reference question is "What does the author mean by ``Family is of course an elastic word?''"

\subsection{Model \& Tools}

For our experiments, we select GPT-J-6B \citep{gpt-j:4} pulled from HuggingFace. We choose this model as it performs well in a zero-shot setting \citep{gpt-j:4} and for its manageable parameter size (6 billion) and open-source code, allowing us to compare performance between a fine-tuned model and a model used in a few-shot setting. 

During decoding, we use a \textbf{top-k} of 50, a \textbf{top-p} of 1.0 and a \textbf{temperature} value of 1.0 after hyperparameter turning. We set a \textbf{length penalty} of 0.8 to favor shorter sequences. As part of prompting, we experiment with multi-example prompting (few shot learning), receiving the best results with the most number of example QA pairs passed. See Appendix for all considered hyperparameter ranges.  

For difficulty classification, we use RoBERTa \citep{roberta:12}. The model is trained with a classification head, taking in a combined input of context, question, and answer with the difficulty labels of 'hard' and 'easy'. Training is performed for 3781 examples, evaluation on 222 examples, and testing on 197 samples. The data is preprocessed to follow a similar ruleset to \citet{Difficulty:3}, with some adjustments.

We perform model training on NYU HPC. All code can be found in our \href{https://github.com/rbolline/Neural-Question-Generation-with-GPT-J}{Github repository}. 


\subsection{Evaluation}

Generated questions are evaluated along three dimensions: fluency, relevancy, and difficulty. Generally, we use a mix of manual human-centric evaluation, automatic metrics and Machine-Learned metrics as described above.

\textbf{Fluency} determines weather a generated question is easy to read and understand, without taking the source (passage or reference question) into account, and is manually evaluated on a 0-2 Likert scale (ranging from poor to excellent). \textbf{Relevancy} for the answer-focused setting determines whether the generated questions and input reference questions are topically related \citep{relevancy:14}, and is measured using manual evaluation on a 0-1 Likert scale (either relevant or irrelevant) as well as with evaluation metrics including BLEU \citep{BLEU:6}, ROUGE-L \citep{rouge:5}, sacreBLEU, and BLEURT-20 \citep{bleurt:7}, a learned evaluation metric that captures the semantic similarities between two pieces of text. These untrained automatic metrics are computed between the reference question from RACE and the question by GPT-J. For the general setting, Relevancy is only determined through manual evaluation by judging the relevancy of the generated question to the context passage. \textbf{Difficulty} is determined by checking if the generated question’s difficulty matches that of the input difficulty fed to the model, and is measured as a Machine-Learned metric using a RoBERTa \citep{roberta:12} classifier trained on RACE to classify questions as being either easy or hard. Our difficulty classifier achieves an --\% F1 score trained on ---- questions and and tested on --- questions. We compare the classifier's output difficulty with the desired difficulty from the input, and evaluate the F1 score.  

\section{Results}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Metric} & \textbf{Finetuned} & \textbf{Few-Shot}\\
\hline
Fluency & & \\ 
\hspace{0.5cm} Manual & - & 1.27 \\
Relevancy & & \\
\hspace{0.5cm} Manual & - & 0.18 \\
\hspace{0.5cm} BLEU-1 & - & 1.40 \\
\hspace{0.5cm} BLEU-2 & - & 0.00 \\
\hspace{0.5cm} BLEU-3 & - & 0.00 \\
\hspace{0.5cm} BLEU-4 & - & 0.00 \\
\hspace{0.5cm} ROUGE-L & - & 16.0 \\
\hspace{0.5cm} BLEURT & - & 32.31 \\
\hspace{0.5cm} sacreBLEU & - & - \\
Difficulty & & \\
\hspace{0.5cm} RoBERTa & - & - \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Metric} & \textbf{Finetuned} & \textbf{Few-Shot}\\
\hline
Fluency & & \\ 
\hspace{0.5cm} Manual & - & - \\
Relevancy & & \\
\hspace{0.5cm} Manual & - & - \\
Difficulty & & \\
\hspace{0.5cm} RoBERTa & - & - \\
\hline
\end{tabular}
\caption{\label{citation-guide}
Evaluation results for the answer focused (top) and general (bottom) settings on GPT-J. Manual fluency and relevancy scores are averages of manual human evaluation scores. Further untrained metrics are included for relevancy. Difficulty is the F1 score achieved by the difficulty classifier.  
}
\end{table}

Results on all three evaluation metrics are included above. See Table 2 in the Appendix for example generated questions and Table 3 for specifics on prompts used in human evaluations.

\section{Conclusion}

\section{Collaboration}
Credit to Sukrit Rao for formulating the project question. He developed and executed the inference scripts for the GPT-J question generating models, prompt design, and developed scripts for the automated evaluation metrics.
Credit to Rohith Bollineni for writing the training/evaluation script, building the difficulty classification model.
Credit to Hasan Khan for question preprocessing, a majority of the paper writing, and pulling references together. All team members worked on manual evaluations of the generated questions. 




\section*{Acknowledgements}

We would like to thank our Professor Sam Bowman and TA Richard Pang for their assistance and guidance through the development of this paper.

\newpage
% Entries for the entire Anthology, followed by custom entries
\bibliography{citations}
\bibliographystyle{acl_natbib}

\clearpage


\begin{table*}[!hbt]
    \centering
    \begin{tabular}{|p{3cm}|p{4cm}|p{4cm}|p{3.5cm}|}
        \hline
        \textbf{Desired Difficulty} & \textbf{Reference Question} & \textbf{Generated Question} & \textbf{Answer}\\
        \hline
        Hard & 
        What is special about the pens produced in 1234 PENS? & 
        Why would you choose 1234 PENS? &  
        Their material is less harmful to the earth\\
        \hline
        Hard & 
        What would be the best title for the passage?  & 
        How is it to climb from the Tibetan side of Mt. Everest? & 
        California teen becomes youngest to climb Mt. Everest\\
        \hline
        Easy & 
        What can be the best title for the passage ? & 
        What do you think is the moral of the story? & 
        Love Saves All\\
        \hline
    \end{tabular}
    \caption{\label{citation-guide}
    Questions generated by GPT-J in a one-shot setting}
    \end{table*}

\begin{table*}[!hbt]
\centering
\begin{tabular}{|p{1.5cm}|p{1cm}|p{3cm}|p{3cm}|p{3cm}|p{1.5cm}|p{1.5cm}|}
\hline
\textbf{Example ID} & \textbf{Article} & \textbf{Answer} & \textbf{Question} & \textbf{Generated Question} & \textbf{Fluency} & \textbf{Relevancy} \\
\hline
middle70-2 & article & Because she wanted to keep the fact a secret	& Why did the boy's mother never cut her hair? & Difficulty: Easy. Answer: Because she wanted to keep the fact a secret. Question: If the main title given in the passage is :, why?	& \{0, 1, 2\} & \{0, 1\}  \\
\hline

\end{tabular}
\captionsetup{margin={0.1cm}}
\caption{\label{citation-guide} 
An example row from the manual evaluation worksheet. The information provided in each row is used by human raters to score the generated question on Fluency and Relevancy. A Google Sheets file loaded with all the generated questions and associated metadata is utilized. All manual evaluation was conducted by the three authors of this paper. }

\end{table*}

\appendix
\section{Appendix}

\clearpage


\end{document}